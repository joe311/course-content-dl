
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Tutorial 1: PyTorch — Neuromatch Academy: Deep Learning</title>
<link href="../../../_static/css/theme.css" rel="stylesheet"/>
<link href="../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<link as="script" href="../../../_static/js/index.1c5a1a01449ed65a7b51.js" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script src="../../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
<script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
<script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../_static/nma-logo-square-4xp.jpg" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="../../W1D2_LinearDeepLearning/chapter_title.html" rel="next" title="Linear Deep Learning"/>
<link href="../chapter_title.html" rel="prev" title="Basics And Pytorch"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</link></link></link></link></head>
<body data-offset="80" data-spy="scroll" data-target="#bd-toc-nav">
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
<div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<img alt="logo" class="logo" src="../../../_static/nma-dl-logo-square-4xp.jpeg"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Deep Learning</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main navigation" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
   Introduction
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using Discord
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  The Basics
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Basics And Pytorch (W1D1)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Tutorial 1: PyTorch
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/chapter_title.html">
   Linear Deep Learning (W1D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial1.html">
     Tutorial 1: Gradient Descent and AutoGrad
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial2.html">
     Tutorial 2: Learning Hyperparameters
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial3.html">
     Tutorial 3: Deep linear neural networks
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/chapter_title.html">
   Multi Layer Perceptrons (W1D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial1.html">
     Tutorial 1: Biological vs. Artificial neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial2.html">
     Tutorial 2: Deep MLPs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D4_Optimization/chapter_title.html">
   Optimization (W1D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_Optimization/student/W1D4_Tutorial1.html">
     Tutorial 1: Optimization techniques
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D5_Regularization/chapter_title.html">
   Regularization (W1D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial1.html">
     Tutorial 1: Regularization techniques part 1
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial2.html">
     Tutorial 2: Regularization techniques part 2
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Doing more with fewer parameters
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/chapter_title.html">
   Convnets And Recurrent Neural Networks (W2D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial1.html">
     Tutorial 1: Introduction to CNNs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial2.html">
     Tutorial 2: Training loop of CNNs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial3.html">
     Tutorial 3: Introduction to RNNs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_ModernConvnets/chapter_title.html">
   Modern Convnets (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial1.html">
     Tutorial 1: Learn how to use modern convnets
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial2.html">
     Tutorial 2: Facial recognition using modern convnets
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/chapter_title.html">
   Modern Recurrent Neural Networks (W2D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.html">
     Tutorial 1: Modeling sequencies and encoding text
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.html">
     Tutorial 2: Modern RNNs and their variants
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/chapter_title.html">
   Attention And Transformers (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/student/W2D4_Tutorial1.html">
     Tutorial 1: Learn how to work with Transformers
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D5_GenerativeModels/chapter_title.html">
   Generative Models (W2D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial1.html">
     Tutorial 1: Variational Autoencoders (VAEs)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial2.html">
     Tutorial 2: Introduction to GANs and Density Ratio Estimation Perspective of GANs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial3.html">
     Tutorial 3: Conditional GANs and Implications of GAN Technology
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Advanced topics
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D1_UnsupervisedAndSelfSupervisedLearning/chapter_title.html">
   Unsupervised And Self Supervised Learning (W3D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
<label for="toctree-checkbox-14">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.html">
     Tutorial 1: Un/Self-supervised learning methods
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D2_BasicReinforcementLearning/chapter_title.html">
   Basic Reinforcement Learning (W3D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
<label for="toctree-checkbox-15">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_BasicReinforcementLearning/student/W3D2_Tutorial1.html">
     Tutorial 1: Introduction to Reinforcement Learning
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D3_ReinforcementLearningForGames/chapter_title.html">
   Reinforcement Learning For Games (W3D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
<label for="toctree-checkbox-16">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_ReinforcementLearningForGames/student/W3D3_Tutorial1.html">
     Tutorial 1: Learn to play games with RL
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D4_ContinualLearning/chapter_title.html">
   Continual Learning (W3D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
<label for="toctree-checkbox-17">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ContinualLearning/student/W3D4_Tutorial1.html">
     Tutorial 1: Introduction to Continual Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ContinualLearning/student/W3D4_Tutorial2.html">
     Tutorial 2: Out-of-distribution (OOD) Learning
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/README.html">
   Introduction to projects
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_guidance.html">
   Daily guide for projects
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
<label for="toctree-checkbox-18">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_1through2_DL.html">
     Modeling Steps 1 - 2
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_3through4_DL.html">
     Modeling Steps 3 - 4
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_5through6_DL.html">
     Modeling Steps 5 - 6
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_7through9_DL.html">
     Modeling Steps 7 - 9
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_10_DL.html">
     Modeling Steps 10
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionDataProjectDL.html">
     Example Data Project: the Train Illusion
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/code/intro.html">
   Notebook with codes for projects
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
<label for="toctree-checkbox-19">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/code/segmentation_denoising.html">
     segmentation + denoising
    </a>
</li>
</ul>
</li>
</ul>
</div>
</nav> <!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
<div class="topbar container-xl fixed-top">
<div class="topbar-contents row">
<div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
<div class="col pl-md-4 topbar-main">
<button aria-controls="site-navigation" aria-expanded="true" aria-label="Toggle navigation" class="navbar-toggler ml-0" data-placement="left" data-target=".site-navigation" data-toggle="tooltip" id="navbar-toggler" title="Toggle navigation" type="button">
<i class="fas fa-bars"></i>
<i class="fas fa-arrow-left"></i>
<i class="fas fa-arrow-up"></i>
</button>
<div class="dropdown-buttons-trigger">
<button aria-label="Download this page" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fas fa-download"></i></button>
<div class="dropdown-buttons">
<!-- ipynb file if we had a myst markdown file -->
<!-- Download raw file -->
<a class="dropdown-buttons" href="../../../_sources/tutorials/W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.ipynb"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Download source file" type="button">.ipynb</button></a>
<!-- Download PDF via print -->
<button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" id="download-print" onclick="window.print()" title="Print to PDF" type="button">.pdf</button>
</div>
</div>
<!-- Source interaction buttons -->
<div class="dropdown-buttons-trigger">
<button aria-label="Connect with source repository" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fab fa-github"></i></button>
<div class="dropdown-buttons sourcebuttons">
<a class="repository-button" href="https://github.com/NeuromatchAcademy/course-content-dl"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Source repository" type="button"><i class="fab fa-github"></i>repository</button></a>
<a class="issues-button" href="https://github.com/NeuromatchAcademy/course-content-dl/issues/new?title=Issue%20on%20page%20%2Ftutorials/W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.html&amp;body=Your%20issue%20content%20here."><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Open an issue" type="button"><i class="fas fa-lightbulb"></i>open issue</button></a>
</div>
</div>
<!-- Full screen (wrap in <a> to have style consistency -->
<a class="full-screen-button"><button aria-label="Fullscreen mode" class="btn btn-secondary topbarbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode" type="button"><i class="fas fa-expand"></i></button></a>
<!-- Launch buttons -->
</div>
<!-- Table of contents -->
<div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
            </div>
<nav id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 1: PyTorch
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-slides">
     Tutorial slides
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#install-dependencies">
     Install dependencies
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure Settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper Functions
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-welcome-to-neuromatch-deep-learning-course">
   Section 1: Welcome to Neuromatch Deep learning course
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-welcome-and-history">
     Video 1: Welcome and History
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-why-dl-is-cool">
     Video 2: Why DL is cool
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-the-basics-of-pytorch">
   Section 2: The Basics of PyTorch
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-creating-tensors">
     Section 2.1: Creating Tensors
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-making-tensors">
       Video 3: Making Tensors
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#numpy-like-number-ranges">
<strong>
      Numpy-like number ranges:
     </strong>
</a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-1-creating-tensors">
       Coding Exercise 2.1: Creating Tensors
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-operations-in-pytorch">
     Section 2.2: Operations in PyTorch
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-4-tensor-operators">
       Video 4: Tensor Operators
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-2-simple-tensor-operations">
       Coding Exercise 2.2 : Simple tensor operations
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-3-manipulating-tensors-in-pytorch">
     Section 2.3 Manipulating Tensors in Pytorch
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-5-tensor-indexing">
       Video 5: Tensor Indexing
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-3-manipulating-tensors">
       Coding Exercise 2.3: Manipulating Tensors
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-4-gpus">
     Section 2.4: GPUs
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-6-gpu-vs-cpu">
       Video 6: GPU vs CPU
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-4-just-how-much-faster-are-gpus">
       Coding Exercise 2.4: Just how much faster are GPUs?
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-5-datasets-and-dataloaders">
     Section 2.5: Datasets and Dataloaders
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-7-getting-data">
       Video 7: Getting Data
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-5-display-an-image-from-the-dataset">
       Coding Exercise 2.5: Display an image from the dataset
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-8-train-and-test">
         Video 8: Train and Test
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-9-data-augmentation-transformations">
         Video 9: Data Augmentation - Transformations
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-6-load-the-cifar10-dataset-as-grayscale-images">
       Coding Exercise 2.6: Load the CIFAR10 dataset as grayscale images
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-neural-networks">
   Section 3:  Neural Networks
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-10-csv-files">
     Video 10: CSV Files
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-1-data-loading">
     Section 3.1: Data Loading
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#generate-sample-data">
       Generate sample data
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-2-create-a-simple-neural-network">
     Section 3.2: Create a Simple Neural Network
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-11-generating-the-neural-network">
       Video 11: Generating the Neural Network
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-2-classify-some-samples">
       Coding Exercise 3.2: Classify some samples
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-3-train-your-neural-network">
     Section 3.3: Train Your Neural Network
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-12-train-the-network">
       Video 12: Train the Network
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-function-to-plot-the-decision-boundary">
       Helper function to plot the decision boundary
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#visualize-the-training-process">
       Visualize the training process
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#execute-this-cell">
       Execute this cell!
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-13-play-with-it">
       Video 13: Play with it
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-3-3-tweak-your-network">
       Exercise 3.3: Tweak your Network
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-14-xor-widget">
         Video 14: XOR Widget
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-3-3-solving-xor">
       Interactive Demo 3.3: Solving XOR
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-4-ethics">
   Section 4: Ethics
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-15-ethics">
     Video 15: Ethics
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus">
   Bonus
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-16-be-a-group">
     Video 16: Be a group
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-17-it-s-a-wrap">
     Video 17: It’s a wrap!
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-18-syllabus">
     Video 18: Syllabus
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#appendix">
   Appendix
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#official-pytorch-resources">
     Official PyTorch resources:
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorials">
       Tutorials
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#documentation">
       Documentation
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#google-colab-resources">
     Google Colab Resources:
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#books-for-reference">
     Books for reference:
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="row" id="main-content">
<div class="col-12 col-md-9 pl-md-3 pr-md-0">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<div class="section" id="tutorial-1-pytorch">
<h1>Tutorial 1: PyTorch<a class="headerlink" href="#tutorial-1-pytorch" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 1, Day 1: Basics and PyTorch</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Shubh Pachchigar, Vladimir Haltakov, Matthew Sargent, Konrad Kording</p>
<p><strong>Content reviewers:</strong> Kelson Shilling-Scrivo, Deepak Raya, Siwei Bai</p>
<p><strong>Content editors:</strong> Anoop Kulkarni, Spiros Chavlis</p>
<p><strong>Production editors:</strong> Arush Tagade, Spiros Chavlis</p>
<p><strong>Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs</strong></p>
<p align="center"><img src="https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True"/></p></div>
<hr class="docutils"/>
<div class="section" id="tutorial-objectives">
<h1>Tutorial Objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this headline">¶</a></h1>
<p>Then have a few specific objectives for this tutorial:</p>
<ul class="simple">
<li><p>Learn about PyTorch and tensors</p></li>
<li><p>Tensor Manipulations</p></li>
<li><p>Data Loading</p></li>
<li><p>GPUs and Cuda Tensors</p></li>
<li><p>Train NaiveNet</p></li>
<li><p>Get to know your pod</p></li>
<li><p>Start thinking about the course as a whole</p></li>
</ul>
<div class="section" id="tutorial-slides">
<h2>Tutorial slides<a class="headerlink" href="#tutorial-slides" title="Permalink to this headline">¶</a></h2>
<p>These are the slides for the videos in this tutorial today</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe allowfullscreen="" frameborder="0" height="480" src="https://mfr.ca-1.osf.io/render?url=https://osf.io/wcjrv/?direct%26mode=render%26action=download%26mode=render" width="854"></iframe>
</div></div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<p>Throughout your Neuromatch tutorials, most (probably all!) notebooks contain setup cells. These cells will import the required Python packages (e.g., PyTorch, NumPy); set global or environment variables, and load in helper functions for things like plotting.</p>
<p>Be sure to run all of the cells in the setup section. Feel free to expand them and have a look at what you are loading in, but you should be able to fulfill the learning objectives of every tutorial without having to look at these cells.</p>
<p>If you start building your own projects built on this code base we highly recommend looking at them in more detail.</p>
<div class="section" id="install-dependencies">
<h2>Install dependencies<a class="headerlink" href="#install-dependencies" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Install dependencies</span>
<span class="o">!</span>pip install pandas --quiet
<span class="o">!</span>pip install -U scikit-learn --quiet
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.11/x64/bin/python -m pip install --upgrade pip' command.</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.11/x64/bin/python -m pip install --upgrade pip' command.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">ToTensor</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h2>Figure Settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure Settings</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="helper-functions">
<h2>Helper Functions<a class="headerlink" href="#helper-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper Functions</span>
<span class="k">def</span> <span class="nf">checkExercise1</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">D</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Helper function for checking exercise.</span>

<span class="sd">  Args:</span>
<span class="sd">    A: torch.Tensor</span>
<span class="sd">    B: torch.Tensor</span>
<span class="sd">    C: torch.Tensor</span>
<span class="sd">    D: torch.Tensor</span>
<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>
  <span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="c1">#TODO better errors and error handling</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">21</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">int</span><span class="p">)):</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Got: </span><span class="si">{</span><span class="n">A</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2"> Expected: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">21</span><span class="p">)</span><span class="si">}</span><span class="s2"> (shape: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">21</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">)"</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span> <span class="n">B</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">np</span><span class="o">.</span><span class="n">vander</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="mi">4</span><span class="p">)):</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"B is not a tensor containing the elements of Z "</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">C</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">21</span><span class="p">):</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"C is not the correct shape "</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">41</span><span class="p">,</span><span class="n">step</span><span class="o">=</span><span class="mi">2</span><span class="p">)):</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"D does not contain the correct elements"</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">errors</span> <span class="o">==</span> <span class="p">[]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"All correct!"</span><span class="p">)</span>

  <span class="k">else</span><span class="p">:</span>
    <span class="p">[</span><span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">errors</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">timeFun</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">iterations</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
  <span class="n">iterations</span> <span class="o">=</span> <span class="n">iterations</span>
  <span class="n">t_total</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">f</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">t_total</span> <span class="o">+=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"time taken for </span><span class="si">{</span><span class="n">iterations</span><span class="si">}</span><span class="s2"> iterations of </span><span class="si">{</span><span class="n">f</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">t_total</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Important note: Scratch Code Cells</strong></p>
<p>If you want to quickly try out something or take a look at the data you can use scratch code cells. They allow you to run Python code, but will not mess up the structure of your notebook.</p>
<p>To open a new scratch cell go to <em>Insert</em> → <em>Scratch code cell</em>.</p>
</div>
</div>
<div class="section" id="section-1-welcome-to-neuromatch-deep-learning-course">
<h1>Section 1: Welcome to Neuromatch Deep learning course<a class="headerlink" href="#section-1-welcome-to-neuromatch-deep-learning-course" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-1-welcome-and-history">
<h2>Video 1: Welcome and History<a class="headerlink" href="#video-1-welcome-and-history" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "36882bb4c9aa454c9b991910a488ac50"}
</script></div>
</div>
<p>*This will be an intensive 3 week adventure. We will all learn Deep Learning. In a group. Groups need standards. Read our
<a class="reference external" href="https://docs.google.com/document/d/1eHKIkaNbAlbx_92tLQelXnicKXEcvFzlyzzeWjEtifM/edit?usp=sharing">Code of Conduct</a>.</p>
</div>
<div class="section" id="video-2-why-dl-is-cool">
<h2>Video 2: Why DL is cool<a class="headerlink" href="#video-2-why-dl-is-cool" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "b9d93a0d48c74f04984a95e4036b33f0"}
</script></div>
</div>
<p><strong>Describe what you hope to get out of this course in about 100 words.</strong></p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-2-the-basics-of-pytorch">
<h1>Section 2: The Basics of PyTorch<a class="headerlink" href="#section-2-the-basics-of-pytorch" title="Permalink to this headline">¶</a></h1>
<p>PyTorch is a Python-based scientific computing package targeted at two sets of
audiences:</p>
<ul class="simple">
<li><p>A replacement for NumPy to use the power of GPUs</p></li>
<li><p>A deep learning platform that provides significant flexibility
and speed</p></li>
</ul>
<p>At its core, PyTorch provides a few key features:</p>
<ul class="simple">
<li><p>A multidimensional <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html">Tensor</a> object, similar to <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html">NumPy Array</a> but with GPU acceleration.</p></li>
<li><p>An optimized <strong>autograd</strong> engine for automatically computing derivatives.</p></li>
<li><p>A clean, modular API for building and deploying <strong>deep learning models</strong>.</p></li>
</ul>
<p>You can find more information about PyTorch in the appendix.</p>
<div class="section" id="section-2-1-creating-tensors">
<h2>Section 2.1: Creating Tensors<a class="headerlink" href="#section-2-1-creating-tensors" title="Permalink to this headline">¶</a></h2>
<div class="section" id="video-3-making-tensors">
<h3>Video 3: Making Tensors<a class="headerlink" href="#video-3-making-tensors" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "8645d57c94684132997d805a2d8bfabf"}
</script></div>
</div>
<p>There are various ways of creating tensors, and when doing any real deep learning project we will usually have to do so.</p>
<p><strong>Construct tensors directly:</strong></p>
<hr class="docutils"/>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we can construct a tensor directly from some common python iterables,</span>
<span class="c1"># such as list and tuple nested iterables can also be handled as long as the</span>
<span class="c1"># dimensions make sense</span>

<span class="c1"># tensor from a list</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="c1">#tensor from a tuple of tuples</span>
<span class="n">b</span> <span class="o">=</span> <span class="p">((</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="c1"># tensor from a numpy array</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Tensor a:"</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Tensor b:"</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Tensor c:"</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tensor a: tensor([0, 1, 2])
Tensor b: tensor([[1.0000, 1.1000],
        [1.2000, 1.3000]])
Tensor c: tensor([[1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
</pre></div>
</div>
</div>
</div>
<p><strong>Some common tensor constructors:</strong></p>
<hr class="docutils"/>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># the numerical arguments we pass to these constructors</span>
<span class="c1"># determine the shape of the output tensor</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Tensor x:"</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Tensor y:"</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Tensor z:"</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tensor x: tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
Tensor y: tensor([0., 0.])
Tensor z: tensor([[[3.4521e+19, 3.0956e-41, 3.4668e+19, 3.0956e-41, 8.9683e-44]]])
</pre></div>
</div>
</div>
</div>
<p>Notice that <code class="docutils literal notranslate"><span class="pre">.empty()</span></code> does not return zeros, but seemingly random small numbers. Unlike <code class="docutils literal notranslate"><span class="pre">.zeros()</span></code>, which initialises the elements of the tensor with zeros, <code class="docutils literal notranslate"><span class="pre">.empty()</span></code> just allocates the memory. It is hence a bit faster if you are looking to just create a tensor.</p>
<p><strong>Creating random tensors and tensors like other tensors:</strong></p>
<hr class="docutils"/>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># there are also constructors for random numbers</span>

<span class="c1"># uniform distribution</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># normal distribution</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="c1"># there are also constructors that allow us to construct</span>
<span class="c1"># a tensor according to the above constructors, but with</span>
<span class="c1"># dimensions equal to another tensor</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Tensor a: "</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Tensor b: "</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Tensor c: "</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Tensor d: "</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tensor a:  tensor([[8.1021e-04, 3.1243e-01, 9.4788e-01]])
Tensor b:  tensor([[-0.7718, -0.0537, -0.6136,  0.2951],
        [ 0.9223, -1.5268,  0.3485, -0.2395],
        [ 0.5399, -0.2473,  1.3193,  0.0404]])
Tensor c:  tensor([[0., 0., 0.]])
Tensor d:  tensor([[0.2572, 0.2277, 0.5119]])
</pre></div>
</div>
</div>
</div>
<p><em>Reproducibility</em>:</p>
<ul class="simple">
<li><p>PyTorch random number generator: You can use <code class="docutils literal notranslate"><span class="pre">torch.manual_seed()</span></code> to seed the RNG for all devices (both CPU and CUDA)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>For custom operators, you might need to set python seed as well:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Random number generators in other libraries</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, we define for you a function called <code class="docutils literal notranslate"><span class="pre">set_seed</span></code> that does the job for you!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Function that controls randomness. NumPy and random modules must be imported.</span>

<span class="sd">  Args:</span>
<span class="sd">    seed : Integer</span>
<span class="sd">      A non-negative integer that defines the random state. Default is `None`.</span>
<span class="sd">    seed_torch : Boolean</span>
<span class="sd">      If `True` sets the random seed for pytorch tensors, so pytorch module</span>
<span class="sd">      must be imported. Default is `True`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">32</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">seed_torch</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Random seed </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s1"> has been set.'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now, let’s use the <code class="docutils literal notranslate"><span class="pre">set_seed</span></code> function in the previous example. Execute the celll multiple times to verify that the numbers printed are always the same.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">simplefun</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">my_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">seed</span><span class="p">:</span>
    <span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">my_seed</span><span class="p">)</span>

  <span class="c1"># uniform distribution</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
  <span class="c1"># normal distribution</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="s2">"Tensor a: "</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Tensor b: "</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">simplefun</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">my_seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Turn `seed` to `False` or change `my_seed`</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random seed 0 has been set.
Tensor a:  tensor([[0.4963, 0.7682, 0.0885]])
Tensor b:  tensor([[ 0.3643,  0.1344,  0.1642,  0.3058],
        [ 0.2100,  0.9056,  0.6035,  0.8110],
        [-0.0451,  0.8797,  1.0482, -0.0445]])
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="numpy-like-number-ranges">
<h2><strong>Numpy-like number ranges:</strong><a class="headerlink" href="#numpy-like-number-ranges" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">.arange()</span></code> and <code class="docutils literal notranslate"><span class="pre">.linspace()</span></code> behave how you would expect them to if you are familar with numpy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Tensor a: </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Numpy array b: </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Tensor c: </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Numpy array d: </span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tensor a: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

Numpy array b: [0 1 2 3 4 5 6 7 8 9]

Tensor c: tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000,
        4.5000, 5.0000])

Numpy array d: [0.  0.5 1.  1.5 2.  2.5 3.  3.5 4.  4.5 5. ]
</pre></div>
</div>
</div>
</div>
<div class="section" id="coding-exercise-2-1-creating-tensors">
<h3>Coding Exercise 2.1: Creating Tensors<a class="headerlink" href="#coding-exercise-2-1-creating-tensors" title="Permalink to this headline">¶</a></h3>
<p>Below you will find some incomplete code. Fill in the missing code to construct the specified tensors.</p>
<p>We want the tensors:</p>
<p><span class="math notranslate nohighlight">\(A:\)</span> 20 by 21 tensor consisting of ones</p>
<p><span class="math notranslate nohighlight">\(B:\)</span> a tensor with elements equal to the elements of numpy array <span class="math notranslate nohighlight">\(Z\)</span></p>
<p><span class="math notranslate nohighlight">\(C:\)</span> a tensor with the same number of elements as <span class="math notranslate nohighlight">\(A\)</span> but with values <span class="math notranslate nohighlight">\(
\sim U(0,1)\)</span></p>
<p><span class="math notranslate nohighlight">\(D:\)</span> a 1D tensor containing the even numbers between 4 and 40 inclusive.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tensor_creation</span><span class="p">(</span><span class="n">Z</span><span class="p">):</span>
  <span class="sd">"""A function that creates various tensors.</span>

<span class="sd">  Args:</span>
<span class="sd">    Z (numpy.ndarray): An array of shape</span>

<span class="sd">  Returns:</span>
<span class="sd">    A : 20 by 21 tensor consisting of ones</span>
<span class="sd">    B : a tensor with elements equal to the elements of numpy array  Z</span>
<span class="sd">    C : a tensor with the same number of elements as A but with values ∼U(0,1)</span>
<span class="sd">    D : a 1D tensor containing the even numbers between 4 and 40 inclusive.</span>
<span class="sd">  """</span>
  <span class="c1">#################################################</span>
  <span class="c1">## TODO for students: fill in the missing code</span>
  <span class="c1">## from the first expression</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: say what they should have done"</span><span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="n">A</span> <span class="o">=</span> <span class="o">...</span>
  <span class="n">B</span> <span class="o">=</span> <span class="o">...</span>
  <span class="n">C</span> <span class="o">=</span> <span class="o">...</span>
  <span class="n">D</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">return</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">D</span>


<span class="c1"># numpy array to copy later</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vander</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="mi">4</span><span class="p">)</span>

<span class="c1"># Uncomment below to check your function!</span>
<span class="c1"># A, B, C, D = tensor_creation(Z)</span>
<span class="c1"># checkExercise1(A, B, C, D)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D1_BasicsAndPytorch/solutions/W1D1_Tutorial1_Solution_d99622ef.py"><em>Click for solution</em></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>All correct!
</pre></div>
</div>
</div>
</div>
<div class="section" id="section-2-2-operations-in-pytorch">
<h2>Section 2.2: Operations in PyTorch<a class="headerlink" href="#section-2-2-operations-in-pytorch" title="Permalink to this headline">¶</a></h2>
<p><strong>Tensor-Tensor operations</strong></p>
<p>We can perform operations on tensors using methods under <code class="docutils literal notranslate"><span class="pre">torch.</span></code></p>
<div class="section" id="video-4-tensor-operators">
<h3>Video 4: Tensor Operators<a class="headerlink" href="#video-4-tensor-operators" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "d42beb91cbc84b6abff0c08ba8218280"}
</script></div>
</div>
<p><strong>Tensor-Tensor operations</strong></p>
<p>We can perform operations on tensors using methods under <code class="docutils literal notranslate"><span class="pre">torch.</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># this only works if c and d already exist</span>
<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
<span class="c1">#Pointwise Multiplication of a and b</span>
<span class="n">torch</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[1.0362, 1.1852, 1.3734],
        [1.3051, 1.9320, 1.1759],
        [1.2698, 1.1507, 1.0317],
        [1.2081, 1.9298, 1.7231],
        [1.7423, 1.5263, 1.2437]])
tensor([[0.0362, 0.1852, 0.3734],
        [0.3051, 0.9320, 0.1759],
        [0.2698, 0.1507, 0.0317],
        [0.2081, 0.9298, 0.7231],
        [0.7423, 0.5263, 0.2437]])
</pre></div>
</div>
</div>
</div>
<p>However, in PyTorch most common Python operators are overridden.
The common standard arithmetic operators (+, -, *, /, and **) have all been lifted to elementwise operations</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">/</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="o">**</span><span class="n">y</span>  <span class="c1"># The ** operator is exponentiation</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([ 2,  4,  7, 12]),
 tensor([0, 0, 1, 4]),
 tensor([ 1,  4, 12, 32]),
 tensor([1.0000, 1.0000, 1.3333, 2.0000]),
 tensor([   1,    4,   64, 4096]))
</pre></div>
</div>
</div>
</div>
<p><strong>Tensor Methods</strong></p>
<p>Tensors also have a number of common arithmetic operations built in. A full list of <strong>all</strong> methods can be found  in the appendix (there are a lot!)</p>
<p>All of these operations should have similar syntax to their numpy equivalents.(Feel free to skip if you already know this!)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="c1"># sum() - note the axis is the axis you move across when summing</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Sum of every element of x: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Sum of the columns of x: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Sum of the rows of x: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Mean value of all elements of x </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Mean values of the columns of x </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Mean values of the rows of x </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.5846, 0.0332, 0.1387],
        [0.2422, 0.8155, 0.7932],
        [0.2783, 0.4820, 0.8198]])


Sum of every element of x: 4.187318325042725
Sum of the columns of x: tensor([1.1051, 1.3306, 1.7517])
Sum of the rows of x: tensor([0.7565, 1.8509, 1.5800])
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean value of all elements of x 0.46525758504867554
Mean values of the columns of x tensor([0.3684, 0.4435, 0.5839])
Mean values of the rows of x tensor([0.2522, 0.6170, 0.5267])
</pre></div>
</div>
</div>
</div>
<p><strong>Matrix Operations</strong></p>
<p>The <code class="docutils literal notranslate"><span class="pre">@</span></code> symbol is overridden to represent matrix multiplication. You can also use <code class="docutils literal notranslate"><span class="pre">torch.matmul()</span></code> to multiply tensors. For dot multiplication, you can use <code class="docutils literal notranslate"><span class="pre">torch.dot()</span></code>, or manipulate the axes of your tensors and do matrix multiplication (we will cover that in the next section).</p>
<p>Transposes of 2D tensors are obtained using <code class="docutils literal notranslate"><span class="pre">torch.t()</span></code> or <code class="docutils literal notranslate"><span class="pre">Tensor.t</span></code>. Note the lack of brackets for <code class="docutils literal notranslate"><span class="pre">Tensor.t</span></code> - it is an attribute, not a method.</p>
</div>
<div class="section" id="coding-exercise-2-2-simple-tensor-operations">
<h3>Coding Exercise 2.2 : Simple tensor operations<a class="headerlink" href="#coding-exercise-2-2-simple-tensor-operations" title="Permalink to this headline">¶</a></h3>
<p>Below are two expressions involving operations on matrices.</p>
<div class="math notranslate nohighlight">
\[\begin{split} \textbf{A} = 
\begin{bmatrix}2 &amp;4 \\5 &amp; 7 
\end{bmatrix} 
\begin{bmatrix} 1 &amp;1 \\2 &amp; 3
\end{bmatrix} 
 + 
\begin{bmatrix}10 &amp; 10  \\ 12 &amp; 1 
\end{bmatrix} 
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split} b = 
\begin{bmatrix} 3 \\ 5 \\ 7
\end{bmatrix} \cdot 
\begin{bmatrix} 2 \\ 4 \\ 8
\end{bmatrix}
\end{split}\]</div>
<p>The code block below that computes these expressions using PyTorch is incomplete - fill in the missing lines.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">simple_operations</span><span class="p">(</span><span class="n">a1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">a2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">a3</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
  <span class="c1">################################################</span>
  <span class="c1">## TODO for students:  complete the first computation using the argument matricies</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: fill in the missing code to complete the operation"</span><span class="p">)</span>
  <span class="c1">################################################</span>
  <span class="c1"># multiplication of tensor a1 with tensor a2 and then add it with tensor a3</span>
  <span class="n">answer</span> <span class="o">=</span> <span class="o">...</span>
  <span class="k">return</span> <span class="n">answer</span>


<span class="c1"># Computing expression 1:</span>

<span class="c1"># init our tensors</span>
<span class="n">a1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]])</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="n">a3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="c1">## uncomment to test your function</span>
<span class="c1"># A = simple_operations(a1, a2, a3)</span>
<span class="c1"># print(A)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D1_BasicsAndPytorch/solutions/W1D1_Tutorial1_Solution_51c270eb.py"><em>Click for solution</em></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">24</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">31</span><span class="p">,</span> <span class="mi">27</span><span class="p">]])</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dot_product</span><span class="p">(</span><span class="n">b1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">b2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
  <span class="c1">###############################################</span>
  <span class="c1">## TODO for students:  complete the first computation using the argument matricies</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: fill in the missing code to complete the operation"</span><span class="p">)</span>
  <span class="c1">###############################################</span>
  <span class="c1"># Use torch.dot() to compute the dot product of two tensors</span>
  <span class="n">product</span> <span class="o">=</span> <span class="o">...</span>
  <span class="k">return</span> <span class="n">product</span>


<span class="c1"># Computing expression 2:</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="c1">## Uncomment to test your function</span>
<span class="c1"># b = dot_product(b1, b2)</span>
<span class="c1"># print(b)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D1_BasicsAndPytorch/solutions/W1D1_Tutorial1_Solution_2a69ad55.py"><em>Click for solution</em></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="mi">82</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="section-2-3-manipulating-tensors-in-pytorch">
<h2>Section 2.3 Manipulating Tensors in Pytorch<a class="headerlink" href="#section-2-3-manipulating-tensors-in-pytorch" title="Permalink to this headline">¶</a></h2>
<div class="section" id="video-5-tensor-indexing">
<h3>Video 5: Tensor Indexing<a class="headerlink" href="#video-5-tensor-indexing" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "34b1125c5aca45dcbe03495ed171263b"}
</script></div>
</div>
<p><strong>Indexing</strong></p>
<p>Just as in numpy, elements in a tensor can be accessed by index. As in any numpy array, the first element has index 0 and ranges are specified to include the first but before the last element. We can access elements according to their relative position to the end of the list by using negative indices. Indexing is also referred to as slicing.</p>
<p>For example, [-1] selects the last element; [1:3] selects the second and the third elements, and [:-2] will select all elements excluding the last and second-to-last elements.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
tensor(9)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([1, 2])
tensor([0, 1, 2, 3, 4, 5, 6, 7])
</pre></div>
</div>
</div>
</div>
<p>When we have multidimensional tensors, indexing rules work the same way as numpy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># make a 5D tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">" shape of x[0]:</span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">" shape of x[0][0]:</span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">" shape of x[0][0][0]:</span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> shape of x[0]:torch.Size([2, 3, 4, 5])
 shape of x[0][0]:torch.Size([3, 4, 5])
 shape of x[0][0][0]:torch.Size([4, 5])
</pre></div>
</div>
</div>
</div>
<p><strong>Flatten and reshape</strong></p>
<p>There are various methods for reshaping tensors. It is common to have to express 2D data in 1D format. Similarly, it is also common to have to reshape a 1D tensor into a 2D tensor. We can achieve this with the <code class="docutils literal notranslate"><span class="pre">.flatten()</span></code> and <code class="docutils literal notranslate"><span class="pre">.reshape()</span></code> methods.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Original z: </span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># 2D -&gt; 1D</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Flattened z: </span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># and back to 2D</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Reshaped (3x4) z: </span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original z: 
 tensor([[ 0,  1],
        [ 2,  3],
        [ 4,  5],
        [ 6,  7],
        [ 8,  9],
        [10, 11]])
Flattened z: 
 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
Reshaped (3x4) z: 
 tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
</pre></div>
</div>
</div>
</div>
<p>You will also see the <code class="docutils literal notranslate"><span class="pre">.view()</span></code> methods used a lot to reshape tensors. There is a subtle difference between <code class="docutils literal notranslate"><span class="pre">.view()</span></code> and <code class="docutils literal notranslate"><span class="pre">.reshape()</span></code>, though for now we will just use <code class="docutils literal notranslate"><span class="pre">.reshape()</span></code>. The documentation can be found in the appendix.</p>
<p><strong>Squeezing tensors</strong></p>
<p>When processing batches of data, you will quite often be left with singleton dimensions. e.g. [1,10] or [256, 1, 3]. This dimension can quite easilly mess up your matrix operations if you don’t plan on it being there…</p>
<p>In order to compress tensors along their singleton dimensions we can use the <code class="docutils literal notranslate"><span class="pre">.squeeze()</span></code> method. We can use the <code class="docutils literal notranslate"><span class="pre">.unsqueeze()</span></code> method to do the opposite.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="c1"># printing the zeroth element of the tensor will not give us the first number!</span>

<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"x[0]: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 10])
x[0]: tensor([-0.7391,  0.8027, -0.6817, -0.1335,  0.0658, -0.5919,  0.7670,  0.6899,
         0.3282,  0.5085])
</pre></div>
</div>
</div>
</div>
<p>Because of that pesky singleton dimension, x[0] gave us the first row instead!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># lets get rid of that singleton dimension and see what happens now</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"x[0]: </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([10])
x[0]: -0.7390837073326111
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># adding singleton dimensions works a similar way, and is often used when tensors</span>
<span class="c1"># being added need same number of dimensions</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"shape of y: </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># lets insert a singleton dimension</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"shape of y: </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>shape of y: torch.Size([5, 5])
shape of y: torch.Size([5, 1, 5])
</pre></div>
</div>
</div>
</div>
<p><strong>Permutation</strong>
Sometimes our dimensions will be in the wrong order! For example, we may be dealing with RGB images with dim [3x48x64], but our pipeline expects the colour dimension to be the last dimension i.e. [48x64x3]. To get around this we can use <code class="docutils literal notranslate"><span class="pre">.permute()</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># `x` has dimensions [color,image_height,image_width]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>

<span class="c1"># we want to permute our tensor to be [ image_height , image_width , color ]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="c1"># permute(1,2,0) means:</span>
<span class="c1"># the 0th dim of my new tensor = the 1st dim of my old tensor</span>
<span class="c1"># the 1st dim of my new tensor = the 2nd</span>
<span class="c1"># the 2nd dim of my new tensor = the 0th</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([48, 64, 3])
</pre></div>
</div>
</div>
</div>
<p>You may also see <code class="docutils literal notranslate"><span class="pre">.transpose()</span></code> used. This works in a similar way as permute, but can only swap two dimensions at once.</p>
<p><strong>Concatenation</strong></p>
<p>In this example, we concatenate two matrices along rows (axis 0, the first element of the shape) vs. columns (axis 1, the second element of the shape). We can see that the first output tensor’s axis-0 length ( 6 ) is the sum of the two input tensors’ axis-0 lengths ( 3+3 ); while the second output tensor’s axis-1 length ( 8 ) is the sum of the two input tensors’ axis-1 lengths ( 4+4 ).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create two tensors of the same shape</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>


<span class="c1">#concatenate them along rows</span>
<span class="n">cat_rows</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># concatenate along columns</span>
<span class="n">cat_cols</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># printing outputs</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Concatenated by rows: shape</span><span class="si">{}</span><span class="s1"> </span><span class="se">\n</span><span class="s1"> </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">cat_rows</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">cat_rows</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1"> Concatenated by colums: shape</span><span class="si">{}</span><span class="s1">  </span><span class="se">\n</span><span class="s1"> </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">cat_cols</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">cat_cols</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Concatenated by rows: shape[6, 4] 
 tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [ 2.,  1.,  4.,  3.],
        [ 1.,  2.,  3.,  4.],
        [ 4.,  3.,  2.,  1.]])

 Concatenated by colums: shape[3, 8]  
 tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])
</pre></div>
</div>
</div>
</div>
<p><strong>Conversion to Other Python Objects</strong></p>
<p>Converting to a NumPy tensor, or vice versa, is easy. The converted result does not share memory. This minor inconvenience is actually quite important: when you perform operations on the CPU or on GPUs, you do not want to halt computation, waiting to see whether the NumPy package of Python might want to be doing something else with the same chunk of memory.</p>
<p>When converting to a numpy array, the information being tracked by the tensor will be lost i.e. the computational graph. This will be covered in detail when you are introduced to autograd tomorrow!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"x: </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">  |  x type:  </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">type</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"y: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">  |  y type:  </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"z: </span><span class="si">{</span><span class="n">z</span><span class="si">}</span><span class="s2">  |  z type:  </span><span class="si">{</span><span class="n">z</span><span class="o">.</span><span class="n">type</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x: tensor([ 0.2659, -0.5148, -0.0613,  0.5046,  0.1385])  |  x type:  torch.FloatTensor
y: [ 0.26593232 -0.5148316  -0.06128114  0.5046449   0.13848118]  |  y type:  &lt;class 'numpy.ndarray'&gt;
z: tensor([ 0.2659, -0.5148, -0.0613,  0.5046,  0.1385])  |  z type:  torch.FloatTensor
</pre></div>
</div>
</div>
</div>
<p>To convert a size-1 tensor to a Python scalar, we can invoke the item function or Python’s built-in functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.5</span><span class="p">])</span>
<span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="nb">float</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([3.5000]), 3.5, 3.5, 3)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="coding-exercise-2-3-manipulating-tensors">
<h3>Coding Exercise 2.3: Manipulating Tensors<a class="headerlink" href="#coding-exercise-2-3-manipulating-tensors" title="Permalink to this headline">¶</a></h3>
<p>Using a combination of the methods discussed above, complete the functions below.</p>
<p><strong>Function A</strong></p>
<p>This function takes in two 2D tensors <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> and returns the column sum of A multiplied by the sum of all the elmements of <span class="math notranslate nohighlight">\(B\)</span> i.e. a scalar, e.g.:</p>
<p><span class="math notranslate nohighlight">\( A = \begin{bmatrix}
1 &amp; 1 \\
1 &amp; 1 
\end{bmatrix}\)</span> <span class="math notranslate nohighlight">\( B = \begin{bmatrix}
1 &amp; 2 &amp; 3\\
1 &amp; 2 &amp; 3 
\end{bmatrix}\)</span>
<span class="math notranslate nohighlight">\( Out =  \begin{bmatrix} 2 &amp; 2 \\
\end{bmatrix} \cdot 12 = \begin{bmatrix}
24 &amp; 24\\
\end{bmatrix}\)</span></p>
<p><strong>Function B</strong></p>
<p>This function takes in a square matrix <span class="math notranslate nohighlight">\(C\)</span> and returns a 2D tensor consisting of a flattened <span class="math notranslate nohighlight">\(C\)</span> with the index of each element appended to this tensor in the row dimension, e.g.:</p>
<p><span class="math notranslate nohighlight">\( C = \begin{bmatrix}
2 &amp; 3 \\
-1 &amp; 10 
\end{bmatrix}\)</span>
<span class="math notranslate nohighlight">\( Out = \begin{bmatrix}
0 &amp; 2 \\
1 &amp; 3 \\
2 &amp; -1 \\
3 &amp; 10
\end{bmatrix}\)</span></p>
<p><strong>Hint:</strong> pay close attention to singleton dimensions</p>
<p><strong>Function C</strong></p>
<p>This function takes in two 2D tensors <span class="math notranslate nohighlight">\(D\)</span> and <span class="math notranslate nohighlight">\(E\)</span>. If the dimensions allow it, this function returns the elementwise sum of <span class="math notranslate nohighlight">\(D\)</span>-shaped <span class="math notranslate nohighlight">\(E\)</span>, and <span class="math notranslate nohighlight">\(D\)</span>; else this function returns a 1D tensor that is the concatenation of the two tensors, e.g.:</p>
<p><span class="math notranslate nohighlight">\( D = \begin{bmatrix}
1 &amp; -1 \\
-1 &amp; 3 
\end{bmatrix}\)</span>
<span class="math notranslate nohighlight">\( E = \begin{bmatrix}
2 &amp; 3 &amp; 0 &amp; 2 \\
\end{bmatrix}\)</span>
<span class="math notranslate nohighlight">\( Out = \begin{bmatrix}
3 &amp; 2 \\
-1 &amp; 5 
\end{bmatrix}\)</span></p>
<p><span class="math notranslate nohighlight">\( D = \begin{bmatrix}
1 &amp; -1 \\
-1 &amp; 3 
\end{bmatrix}\)</span>
<span class="math notranslate nohighlight">\( E = \begin{bmatrix}
2 &amp; 3 &amp; 0  \\
\end{bmatrix}\)</span>
<span class="math notranslate nohighlight">\( Out = \begin{bmatrix}
1 &amp; -1 &amp; -1 &amp; 3  &amp; 2 &amp; 3 &amp; 0  
\end{bmatrix}\)</span></p>
<p><strong>Hint:</strong> <code class="docutils literal notranslate"><span class="pre">torch.numel()</span></code> is an easy way of finding the number of elements in a tensor</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">functionA</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  This function takes in two 2D tensors A and B and returns the column sum of</span>
<span class="sd">  A multiplied by the sum of all the elmements of B, i.e., a scalar.</span>

<span class="sd">  Args:</span>
<span class="sd">    A: torch.Tensor</span>
<span class="sd">    B: torch.Tensor</span>
<span class="sd">  Retuns:</span>
<span class="sd">    output: torch.Tensor</span>
<span class="sd">      The multiplication of the column sum of `A` by the sum of `B`.</span>
<span class="sd">  """</span>
  <span class="c1">################################################</span>
  <span class="c1">## TODO for students: complete functionA</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete function A"</span><span class="p">)</span>
  <span class="c1">################################################</span>
  <span class="c1"># TODO multiplication the sum of the tensors</span>
  <span class="n">output</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">return</span> <span class="n">output</span>


<span class="k">def</span> <span class="nf">functionB</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  This function takes in a square matrix C and returns a 2D tensor consisting of</span>
<span class="sd">  a flattened C with the index of each element appended to this tensor in the</span>
<span class="sd">  row dimension.</span>

<span class="sd">  Args:</span>
<span class="sd">    C: torch.Tensor</span>
<span class="sd">  Retuns:</span>
<span class="sd">    output: torch.Tensor</span>
<span class="sd">      Concatenated tensor.</span>
<span class="sd">  """</span>
  <span class="c1">################################################</span>
  <span class="c1">## TODO for students: complete functionB</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete function B"</span><span class="p">)</span>
  <span class="c1">################################################</span>
  <span class="c1"># TODO flatten the tensor  C</span>
  <span class="n">C</span> <span class="o">=</span> <span class="o">...</span>
  <span class="c1"># TODO create the idx tensor to be concatenated to C</span>
  <span class="n">idx_tensor</span> <span class="o">=</span> <span class="o">...</span>
  <span class="c1"># TODO concatenate the two tensors</span>
  <span class="n">output</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">return</span> <span class="n">output</span>


<span class="k">def</span> <span class="nf">functionC</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  This function takes in two 2D tensors D and E . If the dimensions allow it,</span>
<span class="sd">  this function returns the elementwise sum of D-shaped E, and D;</span>
<span class="sd">  else this function returns a 1D tensor that is the concatenation of the</span>
<span class="sd">  two tensors.</span>

<span class="sd">  Args:</span>
<span class="sd">    D: torch.Tensor</span>
<span class="sd">    E: torch.Tensor</span>
<span class="sd">  Retuns:</span>
<span class="sd">    output: torch.Tensor</span>
<span class="sd">      Concatenated tensor.</span>
<span class="sd">  """</span>
  <span class="c1">################################################</span>
  <span class="c1">## TODO for students: complete functionB</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete function C"</span><span class="p">)</span>
  <span class="c1">################################################</span>
  <span class="c1"># TODO check we can reshape E into the shape of D</span>
  <span class="k">if</span> <span class="o">...</span><span class="p">:</span>
    <span class="c1"># TODO reshape E into the shape of D</span>
    <span class="n">E</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># TODO sum the two tensors</span>
    <span class="n">output</span> <span class="o">=</span> <span class="o">...</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># TODO flatten both tensors</span>
    <span class="n">D</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">E</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># TODO concatenate the two tensors in the correct dimension</span>
    <span class="n">output</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">return</span> <span class="n">output</span>


<span class="c1">## Implement the functions above and then uncomment the following lines to test your code</span>
<span class="c1"># print(functionA(torch.tensor([[1, 1], [1, 1]]), torch.tensor([[1, 2, 3], [1, 2, 3]])))</span>
<span class="c1"># print(functionB(torch.tensor([[2, 3], [-1, 10]])))</span>
<span class="c1"># print(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0, 2]])))</span>
<span class="c1"># print(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0]])))</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D1_BasicsAndPytorch/solutions/W1D1_Tutorial1_Solution_524e1dab.py"><em>Click for solution</em></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([</span><span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">])</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">5</span><span class="p">]])</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="section-2-4-gpus">
<h2>Section 2.4: GPUs<a class="headerlink" href="#section-2-4-gpus" title="Permalink to this headline">¶</a></h2>
<div class="section" id="video-6-gpu-vs-cpu">
<h3>Video 6: GPU vs CPU<a class="headerlink" href="#video-6-gpu-vs-cpu" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "c07d4de3d1e84eb280efc01d8a3b2d80"}
</script></div>
</div>
<p>By default, when we create a tensor it will <em>not</em> live on the GPU!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>cpu
</pre></div>
</div>
</div>
</div>
<p>When using Colab notebooks by default will not have access to a GPU. In order to start using GPUs we need to request one. We can do this by going to the runtime tab at the top of the page.</p>
<p>By following Runtime -&gt; Change runtime type and selecting “GPU” from the Hardware Accelerator dropdown list, we can start playing with sending tensors to GPUs.</p>
<p>Once you have done this your runtime will restart and you will need to rerun the first setup cell to reimport PyTorch. Then proceed to the next cell.</p>
<p>(For more information on the GPU usage policy you can view in the appendix)</p>
<p><strong>Now we have a GPU</strong></p>
<p>The cell below should return True.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
<p>CUDA is an API developed by Nvidia for interfacing with GPUs. PyTorch provides us with a layer of abstraction, and allows us to launch CUDA kernels using pure Python. <em>NOTE I am assuming that GPU stuff might be covered in more detail on another day but there could be a bit more detail here.</em></p>
<p>In short, we get the power of parallising our tensor computations on GPUs, whilst only writing (relatively) simple Python!</p>
<p>Let’s make some CUDA tensors!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">set_device</span><span class="p">():</span>
  <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">!=</span> <span class="s2">"cuda"</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"WARNING: For this notebook to perform best, "</span>
          <span class="s2">"if possible, in the menu under `Runtime` -&gt; "</span>
          <span class="s2">"`Change runtime type.`  select `GPU` "</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"GPU is enabled in this notebook."</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">device</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># common device agnostic way of writing code that can run on cpu OR gpu</span>
<span class="c1"># that we provide for you in each of the tutorials</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="n">set_device</span><span class="p">()</span>

<span class="c1"># we can specify a device when we first create our tensor</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># we can also use the .to() method to change the device a tensor lives on</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"y before calling to() |  device: </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2"> | dtype: </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">type</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"y after calling to() |  device: </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2"> | dtype: </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">type</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -&gt; `Change runtime type.`  select `GPU` 
torch.float32
cpu
y before calling to() |  device: cpu | dtype: torch.FloatTensor
y after calling to() |  device: cpu | dtype: torch.FloatTensor
</pre></div>
</div>
</div>
</div>
<p><strong>Operations between cpu tensors and cuda tensors</strong></p>
<p>Note that the type of the tensor changed after calling <code class="docutils literal notranslate"><span class="pre">.to()</span></code>. What happens if we try and perform operations on tensors on devices?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cpu"</span><span class="p">)</span>

<span class="c1"># Uncomment the following line and run this cell</span>
<span class="c1"># z = x + y</span>
</pre></div>
</div>
</div>
</div>
<p>We cannot combine cuda tensors and cpu tensors in this fashion. If we want to compute an operation that combines tensors on different devices, we need to move them first! We can use the <code class="docutils literal notranslate"><span class="pre">.to()</span></code> method as before, or the <code class="docutils literal notranslate"><span class="pre">.cpu()</span></code> and <code class="docutils literal notranslate"><span class="pre">.cuda()</span></code> methods.</p>
<p>Genrally in this course all Deep learning is done on the GPU and any computation is done on the CPU, so sometimes we have to pass things back and forth so you’ll see us call</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">DEVICE</span><span class="p">)</span>

<span class="c1"># moving to cpu</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>  <span class="c1"># alternatively, you can use x = x.cpu()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># moving to gpu</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>  <span class="c1"># alternatively, you can use y = y.cuda()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([3, 5, 7])
tensor([ 9, 11, 13])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="coding-exercise-2-4-just-how-much-faster-are-gpus">
<h3>Coding Exercise 2.4: Just how much faster are GPUs?<a class="headerlink" href="#coding-exercise-2-4-just-how-much-faster-are-gpus" title="Permalink to this headline">¶</a></h3>
<p>Below is a simple function. Complete the second function, such that it is performs the same operations as the first function, but entirely on the GPU.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dim</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">simpleFun</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Args:</span>
<span class="sd">    dim: integer</span>
<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">z</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>

  <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">z</span>

  <span class="c1"># garbage collection</span>

  <span class="k">del</span> <span class="n">x</span>
  <span class="k">del</span> <span class="n">y</span>
  <span class="k">del</span> <span class="n">z</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">simpleFunGPU</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Args:</span>
<span class="sd">    dim: integer</span>
<span class="sd">    device: "cpu" or "cuda"</span>
<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>
  <span class="c1">###############################################</span>
  <span class="c1">## TODO for students: recreate the above function, but</span>
  <span class="c1">## ensure all computation happens  on the GPU</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: fill in the missing code to create the tensors"</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
  <span class="n">y</span> <span class="o">=</span> <span class="o">...</span>
  <span class="n">z</span> <span class="o">=</span> <span class="o">...</span>

  <span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
  <span class="n">y</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">del</span> <span class="n">x</span>
  <span class="k">del</span> <span class="n">y</span>
  <span class="k">del</span> <span class="n">z</span>


<span class="c1">## TODO: Implement the function above and uncomment the following lines to test your code</span>
<span class="c1"># timeFun(simpleFun, dim=dim, iterations=iterations, device=DEVICE)</span>
<span class="c1"># timeFun(simpleFunGPU, dim=dim, iterations=iterations, device=DEVICE)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to remove solution</span>
<span class="k">def</span> <span class="nf">simpleFunGPU</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Args:</span>
<span class="sd">    dim: integer</span>
<span class="sd">    device: "cpu" or "cuda"</span>
<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span><span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">z</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span><span class="n">dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

  <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">z</span>


  <span class="k">del</span> <span class="n">x</span>
  <span class="k">del</span> <span class="n">y</span>
  <span class="k">del</span> <span class="n">z</span>


<span class="c1">## TODO: Implement the function above and uncomment the following lines to test your code</span>
<span class="n">timeFun</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">simpleFun</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="n">iterations</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">timeFun</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">simpleFunGPU</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="n">iterations</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">DEVICE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>time taken for 1 iterations of simpleFun(10000): 16.81589
</pre></div>
</div>
</div>
</div>
<p><strong>Discuss!</strong></p>
<p>Try and reduce the dimensions of the tensors and increase the iterations. You can get to a point where the cpu only function is faster than the GPU function. Why might this be?</p>
</div>
</div>
<div class="section" id="section-2-5-datasets-and-dataloaders">
<h2>Section 2.5: Datasets and Dataloaders<a class="headerlink" href="#section-2-5-datasets-and-dataloaders" title="Permalink to this headline">¶</a></h2>
<div class="section" id="video-7-getting-data">
<h3>Video 7: Getting Data<a class="headerlink" href="#video-7-getting-data" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
</div>
<p>When training neural network models you will be working with large amounts of data. Fortunately, PyTorch offers some great tools that help you organize and manipulate your data samples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import dataset and dataloaders related packages</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">ToTensor</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">Compose</span><span class="p">,</span> <span class="n">Grayscale</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Datasets</strong></p>
<p>The <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> package gives you easy access to many of the publicly available datasets. Let’s load the <a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR10</a> dataset, which contains color images of 10 different classes, like vehicles and animals.</p>
<p>Creating an object of type <code class="docutils literal notranslate"><span class="pre">datasets.CIFAR10</span></code> will automatically download and load all images from the dataset. The resulting data structure can be treated as a list containing data samples and their corresponding labels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and load the images from the CIFAR10 dataset</span>
<span class="n">cifar10_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s2">"data"</span><span class="p">,</span>            <span class="c1"># path where the images will be stored</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>          <span class="c1"># all images should be downloaded</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">()</span>    <span class="c1"># transform the images to tensors</span>
    <span class="p">)</span>

<span class="c1"># Print the number of samples in the loaded dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of samples:</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">cifar10_data</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Class names:</span><span class="si">{</span><span class="n">cifar10_data</span><span class="o">.</span><span class="n">classes</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We have 50000 samples loaded. Now let’s take a look at one of them in detail. Each sample consists of an image and its corresponding label.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Choose a random sample</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2021</span><span class="p">)</span>
<span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">cifar10_data</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cifar10_data</span><span class="p">))]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Label:'</span><span class="p">,</span> <span class="n">cifar10_data</span><span class="o">.</span><span class="n">classes</span><span class="p">[</span><span class="n">label</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Image size:'</span><span class="p">,</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Color images are modeled as 3 dimensional tensors. The first dimension corresponds to the channels (C) of the image (in this case we have RGB images). The second dimensions is the height (H) of the image and the third is the width (W). We can denote this image format as C × H × W.</p>
</div>
<div class="section" id="coding-exercise-2-5-display-an-image-from-the-dataset">
<h3>Coding Exercise 2.5: Display an image from the dataset<a class="headerlink" href="#coding-exercise-2-5-display-an-image-from-the-dataset" title="Permalink to this headline">¶</a></h3>
<p>Let’s try to display the image using <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>. The code below will not work, because <code class="docutils literal notranslate"><span class="pre">imshow</span></code> expects to have the image in a different format - <span class="math notranslate nohighlight">\(H \times W \times C\)</span>.</p>
<p>You need to reorder the dimensions of the tensor using the <code class="docutils literal notranslate"><span class="pre">permute</span></code> method of the tensor. PyTorch <code class="docutils literal notranslate"><span class="pre">torch.permute(*dims)</span></code> rearranges the original tensor according to the desired ordering and returns a new multidimensional rotated tensor. The size of the returned tensor remains the same as that of the original.</p>
<p><strong>Code hint:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># create a tensor of size 2 x 4</span>
<span class="n">input_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="c1"># print its size and the tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_var</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_var</span><span class="p">)</span>

<span class="c1"># dimensions permuted</span>
<span class="n">input_var</span> <span class="o">=</span> <span class="n">input_var</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="c1"># print its size and the permuted tensor</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_var</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_var</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># TODO: Uncomment the following line to see the error that arises from the current image format</span>
<span class="c1"># plt.imshow(image)</span>

<span class="c1"># TODO: Comment the above line and fix this code by reordering the tensor dimensions</span>
<span class="c1"># plt.imshow(image.permute(...))</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D1_BasicsAndPytorch/solutions/W1D1_Tutorial1_Solution_69b74721.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D1_BasicsAndPytorch/static/W1D1_Tutorial1_Solution_69b74721_1.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D1_BasicsAndPytorch/static/W1D1_Tutorial1_Solution_69b74721_1.png" style="width: 835.0px; height: 827.0px;"/></a>
<div class="section" id="video-8-train-and-test">
<h4>Video 8: Train and Test<a class="headerlink" href="#video-8-train-and-test" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_remove-input docutils container">
</div>
<p><strong>Training and Test Datasets</strong></p>
<p>When loading a dataset, you can specify if you want to load the training or the test samples using the <code class="docutils literal notranslate"><span class="pre">train</span></code> argument. We can load the training and test datasets separately. For simplicity, today we will not use both datasets separately, but this topic will be adressed in the next days.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the training samples</span>
<span class="n">training_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s2">"data"</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">()</span>
    <span class="p">)</span>

<span class="c1"># Load the test samples</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span>
    <span class="n">root</span><span class="o">=</span><span class="s2">"data"</span><span class="p">,</span>
    <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">transform</span><span class="o">=</span><span class="n">ToTensor</span><span class="p">()</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="video-9-data-augmentation-transformations">
<h4>Video 9: Data Augmentation - Transformations<a class="headerlink" href="#video-9-data-augmentation-transformations" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_remove-input docutils container">
</div>
<p><strong>Dataloader</strong></p>
<p>Another important concept is the <code class="docutils literal notranslate"><span class="pre">Dataloader</span></code>. It is a wrapper around the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> that splits it into minibatches (important for training the neural network) and makes the data iterable. The <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> argument is used to shuffle the order of the samples across the minibatches.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create dataloaders with</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><em>Reproducibility:</em> DataLoader will reseed workers following Randomness in multi-process data loading algorithm. Use <code class="docutils literal notranslate"><span class="pre">worker_init_fn()</span></code> and a <code class="docutils literal notranslate"><span class="pre">generator</span></code> to preserve reproducibility:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">seed_worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
    <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>


<span class="n">g_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
<span class="n">g_seed</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">my_seed</span><span class="p">)</span>

<span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
    <span class="n">worker_init_fn</span><span class="o">=</span><span class="n">seed_worker</span><span class="p">,</span>
    <span class="n">generator</span><span class="o">=</span><span class="n">g_seed</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>We can now query the next batch from the data loader and inspect it. For this we need to convert the dataloader object to a Python iterator using the function <code class="docutils literal notranslate"><span class="pre">iter</span></code> and then we can query the next batch using the function <code class="docutils literal notranslate"><span class="pre">next</span></code>.</p>
<p>We can now see that we have a 4D tensor. This is because we have a 64 images in the batch (<span class="math notranslate nohighlight">\(B\)</span>) and each image has 3 dimensions: channels (<span class="math notranslate nohighlight">\(C\)</span>), height (<span class="math notranslate nohighlight">\(H\)</span>) and width (<span class="math notranslate nohighlight">\(W\)</span>). So, the size of the 4D tensor is <span class="math notranslate nohighlight">\(B \times C \times H \times W\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the next batch</span>
<span class="n">batch_images</span><span class="p">,</span> <span class="n">batch_labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Batch size:'</span><span class="p">,</span> <span class="n">batch_images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Display the first image from the batch</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">batch_images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Transformations</strong></p>
<p>Another useful feature when loading a dataset is applying transformations on the data - color conversions, normalization, cropping, rotation etc. There are many predefined transformations in the <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code> package and you can also combine them using the <code class="docutils literal notranslate"><span class="pre">Compose</span></code> transform. Checkout the <a class="reference external" href="https://pytorch.org/vision/stable/transforms.html">pytorch documentation</a> for details.</p>
</div>
</div>
<div class="section" id="coding-exercise-2-6-load-the-cifar10-dataset-as-grayscale-images">
<h3>Coding Exercise 2.6: Load the CIFAR10 dataset as grayscale images<a class="headerlink" href="#coding-exercise-2-6-load-the-cifar10-dataset-as-grayscale-images" title="Permalink to this headline">¶</a></h3>
<p>The goal of this excercise is to load the images from the CIFAR10 dataset as grayscale images. Note that we rerun the <code class="docutils literal notranslate"><span class="pre">set_seed</span></code> function to ensure reproducibility.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">my_data_load</span><span class="p">():</span>
  <span class="c1">###############################################</span>
  <span class="c1">## TODO for students: recreate the above function, but</span>
  <span class="c1">## ensure all computation happens  on the GPU</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: fill in the missing code to load the data"</span><span class="p">)</span>
  <span class="c1">###############################################</span>
  <span class="c1">## TODO Load the CIFAR10 data using a transform that converts the images to grayscale tensors</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="o">...</span><span class="p">,</span>
                          <span class="n">transform</span><span class="o">=...</span><span class="p">)</span>
  <span class="c1"># Display a random grayscale image</span>
  <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))]</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">"gray"</span><span class="p">)</span>


<span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">2021</span><span class="p">)</span>
<span class="c1">## After implementing the above code, uncomment the following lines to test your code</span>
<span class="c1"># my_data_load()</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D1_BasicsAndPytorch/solutions/W1D1_Tutorial1_Solution_8a7b1b66.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D1_BasicsAndPytorch/static/W1D1_Tutorial1_Solution_8a7b1b66_1.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D1_BasicsAndPytorch/static/W1D1_Tutorial1_Solution_8a7b1b66_1.png" style="width: 835.0px; height: 827.0px;"/></a>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-3-neural-networks">
<h1>Section 3:  Neural Networks<a class="headerlink" href="#section-3-neural-networks" title="Permalink to this headline">¶</a></h1>
<p>Now it’s time for you to create your first neural network using PyTorch. This section will walk you through the process of:</p>
<ul class="simple">
<li><p>Creating a simple neural network model</p></li>
<li><p>Training the network</p></li>
<li><p>Visualizing the results of the network</p></li>
<li><p>Tweeking the network</p></li>
</ul>
<div class="section" id="video-10-csv-files">
<h2>Video 10: CSV Files<a class="headerlink" href="#video-10-csv-files" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
<div class="section" id="section-3-1-data-loading">
<h2>Section 3.1: Data Loading<a class="headerlink" href="#section-3-1-data-loading" title="Permalink to this headline">¶</a></h2>
<p>First we need some sample data to train our network on. You can use the function below to generate an example dataset consisting of 2D points along two interleaving half circles. The data will be stored in a file called <code class="docutils literal notranslate"><span class="pre">sample_data.csv</span></code>. You can inspect the file directly in Colab by going to Files on the left side and opening the CSV file.</p>
<div class="section" id="generate-sample-data">
<h3>Generate sample data<a class="headerlink" href="#generate-sample-data" title="Permalink to this headline">¶</a></h3>
<p>we used <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> module</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Generate sample data</span>
<span class="c1"># @markdown we used `scikit-learn` module</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="c1"># Create a dataset of 256 points with a little noise</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Store the data as a Pandas data frame and save it to a CSV file</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">x0</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">x1</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">'sample_data.csv'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can load the data from the CSV file using the Pandas library. Pandas provides many functions for reading files in varios formats. When loading data from a CSV file, we can reference the columns directly by their names.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the data from the CSV file in a Pandas DataFrame</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"sample_data.csv"</span><span class="p">)</span>

<span class="c1"># Create a 2D numpy array from the x0 and x1 columns</span>
<span class="n">X_orig</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s2">"x0"</span><span class="p">,</span> <span class="s2">"x1"</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1"># Create a 1D numpy array from the y column</span>
<span class="n">y_orig</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"y"</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1"># Print the sizes of the generated 2D points X and the corresponding labels Y</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Size X:</span><span class="si">{</span><span class="n">X_orig</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Size y:</span><span class="si">{</span><span class="n">y_orig</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Visualize the dataset. The color of the points is determined by the labels `y_orig`.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_orig</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_orig</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_orig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Prepare Data for PyTorch</strong></p>
<p>Now let’s prepare the data in a format suitable for PyTorch - convert everything into tensors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize the device variable</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="n">set_device</span><span class="p">()</span>

<span class="c1"># Convert the 2D points to a float32 tensor</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">X_orig</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Upload the tensor to the device</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Size X:</span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># Convert the labels to a long interger tensor</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_orig</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">)</span>
<span class="c1"># Upload the tensor to the device</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Size y:</span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="section-3-2-create-a-simple-neural-network">
<h2>Section 3.2: Create a Simple Neural Network<a class="headerlink" href="#section-3-2-create-a-simple-neural-network" title="Permalink to this headline">¶</a></h2>
<div class="section" id="video-11-generating-the-neural-network">
<h3>Video 11: Generating the Neural Network<a class="headerlink" href="#video-11-generating-the-neural-network" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
</div>
<p>For this example we want to have a simple neural network consisting of 3 layers:</p>
<ul class="simple">
<li><p>1 input layer of size 2 (our points have 2 coordinates)</p></li>
<li><p>1 hidden layer of size 16 (you can play with different numbers here)</p></li>
<li><p>1 output layer of size 2 (we want the have the scores for the two classes)</p></li>
</ul>
<p>During the course you will deal with differend kinds of neural networks. On Day 2 we will focus on linear networks, but you will work with some more complicated architectures in the next days. The example here is meant to demonstrate the process of creating and training a neural network end-to-end.</p>
<p><strong>Programing the Network</strong></p>
<p>PyTorch provides a base class for all neural network modules called <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html"><code class="docutils literal notranslate"><span class="pre">nn.Module</span></code></a>. You need to inherit from <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> and implement some important methods:</p>
<p><code class="docutils literal notranslate"><span class="pre">__init__</span></code></p>
<p>In the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method you need to define the structure of your network. Here you will specify what layers will the network consist of, what activation functions will be used etc.</p>
<p><code class="docutils literal notranslate"><span class="pre">forward</span></code></p>
<p>All neural network modules need to implement the <code class="docutils literal notranslate"><span class="pre">forward</span></code> method. It specifies the computations the network needs to do when data is passed through it.</p>
<p><code class="docutils literal notranslate"><span class="pre">predict</span></code></p>
<p>This is not an obligatory method of a neural network module, but it is a good practice if you want to quickly get the most likely label from the network. It calls the <code class="docutils literal notranslate"><span class="pre">forward</span></code> method and chooses the label with the highest score.</p>
<p><code class="docutils literal notranslate"><span class="pre">train</span></code></p>
<p>This is also not an obligatory method, but it is a good practice to have. The method will be used to train the network parameters and will be implemented later in the notebook.</p>
<blockquote>
<div><p>Note that you can use the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method of a module directly and it will invoke the <code class="docutils literal notranslate"><span class="pre">forward</span></code> method: <code class="docutils literal notranslate"><span class="pre">net()</span></code> does the same as <code class="docutils literal notranslate"><span class="pre">net.forward()</span></code>.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Inherit from nn.Module - the base class for neural network modules provided by Pytorch</span>
<span class="k">class</span> <span class="nc">NaiveNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

  <span class="c1"># Define the structure of your network</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">NaiveNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># The network is defined as a sequence of operations</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>  <span class="c1"># Transformation from the input to the hidden layer</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>         <span class="c1"># Activation function (ReLU) is a non-linearity which is widely used because it reduces computation. The function returns 0 if it receives any</span>
                           <span class="c1"># negative input, but for any positive value x, it returns that value back.</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>  <span class="c1"># Transformation from the hidden to the output layer</span>
    <span class="p">)</span>

  <span class="c1"># Specify the computations performed on the data</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># Pass the data through the layers</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="c1"># Choose the most likely label predicted by the network</span>
  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># Pass the data through the networks</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Choose the label with the highest score</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

  <span class="c1"># Train the neural network (will be implemented later)</span>
  <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Check that your network works</strong></p>
<p>Create an instance of your model and visualize it</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create new NaiveNet and transfer it to the device</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">NaiveNet</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

<span class="c1"># Print the structure of the network</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="coding-exercise-3-2-classify-some-samples">
<h3>Coding Exercise 3.2: Classify some samples<a class="headerlink" href="#coding-exercise-3-2-classify-some-samples" title="Permalink to this headline">¶</a></h3>
<p>Now let’s pass some of the points of our dataset through the network and see if it works. You should not expect the network to actually classify the points correctly, because it has not been trained yet.</p>
<p>The goal here is just to get some experience with the data structures that are passed to the forward and predict methods and their results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Get the samples</span>
<span class="c1"># X_samples = ...</span>
<span class="c1"># print("Sample input:", X_samples)</span>

<span class="c1">## Do a forward pass of the network</span>
<span class="c1"># output = ...</span>
<span class="c1"># print("Network output:", output)</span>

<span class="c1">## Predict the label of each point</span>
<span class="c1"># y_predicted = ...</span>
<span class="c1"># print("Predicted labels:", y_predicted)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W1D1_BasicsAndPytorch/solutions/W1D1_Tutorial1_Solution_49a61fb7.py"><em>Click for solution</em></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Sample</span> <span class="nb">input</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.9066</span><span class="p">,</span>  <span class="mf">0.5052</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2024</span><span class="p">,</span>  <span class="mf">1.1226</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.0685</span><span class="p">,</span>  <span class="mf">0.2809</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.6720</span><span class="p">,</span>  <span class="mf">0.5097</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.8548</span><span class="p">,</span>  <span class="mf">0.5122</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cuda:0'</span><span class="p">)</span>
<span class="n">Network</span> <span class="n">output</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.3032</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5563</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1419</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3195</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2879</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6030</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2665</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4831</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.2973</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5369</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cuda:0'</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddmmBackward</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">Predicted</span> <span class="n">labels</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cuda:0'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="section-3-3-train-your-neural-network">
<h2>Section 3.3: Train Your Neural Network<a class="headerlink" href="#section-3-3-train-your-neural-network" title="Permalink to this headline">¶</a></h2>
<div class="section" id="video-12-train-the-network">
<h3>Video 12: Train the Network<a class="headerlink" href="#video-12-train-the-network" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
</div>
<p>Now it is time to train your network on your dataset. Don’t worry if you don’t fully understand everything yet - we wil cover training in much more details in the next days. For now, the goal is just to see your network in action!</p>
<p>You will usually implement the <code class="docutils literal notranslate"><span class="pre">train</span></code> method directly when implementing your class <code class="docutils literal notranslate"><span class="pre">NaiveNet</span></code>. Here, we will implement it as a function outside of the class in order to have it in a ceparate cell.</p>
</div>
<div class="section" id="helper-function-to-plot-the-decision-boundary">
<h3>Helper function to plot the decision boundary<a class="headerlink" href="#helper-function-to-plot-the-decision-boundary" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper function to plot the decision boundary</span>

<span class="c1"># Code adapted from this notebook: https://jonchar.net/notebooks/Artificial-Neural-Network-with-Keras/</span>

<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
  <span class="c1"># Transfer the data to the CPU</span>
  <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

  <span class="c1"># Check if the frames folder exists and create it if needed</span>
  <span class="n">frames_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"frames"</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">frames_path</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
    <span class="n">frames_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">()</span>

  <span class="c1"># Set min and max values and give it some padding</span>
  <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">.5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">.5</span>
  <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">.5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">.5</span>
  <span class="n">h</span> <span class="o">=</span> <span class="mf">0.01</span>

  <span class="c1"># Generate a grid of points with distance h between them</span>
  <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>

  <span class="c1"># Predict the function value for the whole gid</span>
  <span class="n">grid_points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()]</span>
  <span class="n">grid_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">grid_points</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">)</span>
  <span class="n">Z</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">grid_points</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
  <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

  <span class="c1"># Plot the contour and training examples</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">binary</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Implement the train function given a training dataset X and correcsponding labels y</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="c1"># The Cross Entropy Loss is suitable for classification problems</span>
  <span class="n">loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

  <span class="c1"># Create an optimizer (Stochastic Gradient Descent) that will be used to train the network</span>
  <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

  <span class="c1"># Number of epochs</span>
  <span class="n">epochs</span> <span class="o">=</span> <span class="mi">15000</span>

  <span class="c1"># List of losses for visualization</span>
  <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># Pass the data through the network and compute the loss</span>
    <span class="c1"># We'll use the whole dataset during the training instead of using batches</span>
    <span class="c1"># in to order to keep the code simple for now.</span>
    <span class="n">y_logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">y_logits</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Clear the previous gradients and compute the new ones</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Adapt the weights of the network</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Store the loss</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># Print the results at every 1000th epoch</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> loss is </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

      <span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">)</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">'frames/</span><span class="si">{:05d}</span><span class="s1">.png'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">losses</span>


<span class="c1"># Create a new network instance a train it</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">NaiveNet</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">losses</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Plot the loss during training</strong></p>
<p>Plot the loss during the training to see how it reduces and converges.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)),</span> <span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Loss"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="visualize-the-training-process">
<h3>Visualize the training process<a class="headerlink" href="#visualize-the-training-process" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="execute-this-cell">
<h3>Execute this cell!<a class="headerlink" href="#execute-this-cell" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Visualize the training process</span>
<span class="c1"># @markdown ### Execute this cell!</span>
<span class="o">!</span>pip install imageio --quiet
<span class="o">!</span>pip install pathlib --quiet

<span class="kn">import</span> <span class="nn">imageio</span>
<span class="kn">from</span> <span class="nn">IPython.core.interactiveshell</span> <span class="kn">import</span> <span class="n">InteractiveShell</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="n">InteractiveShell</span><span class="o">.</span><span class="n">ast_node_interactivity</span> <span class="o">=</span> <span class="s2">"all"</span>

<span class="c1"># Make a list with all images</span>
<span class="n">images</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="n">filename</span> <span class="o">=</span> <span class="s2">"frames/0"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s2">"000.png"</span>
  <span class="n">images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">imageio</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">filename</span><span class="p">))</span>
<span class="c1"># Save the gif</span>
<span class="n">imageio</span><span class="o">.</span><span class="n">mimsave</span><span class="p">(</span><span class="s1">'frames/movie.gif'</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
<span class="n">gifPath</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"frames/movie.gif"</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">gifPath</span><span class="p">,</span><span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
  <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">(),</span> <span class="nb">format</span><span class="o">=</span><span class="s1">'png'</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="video-13-play-with-it">
<h3>Video 13: Play with it<a class="headerlink" href="#video-13-play-with-it" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
</div>
</div>
<div class="section" id="exercise-3-3-tweak-your-network">
<h3>Exercise 3.3: Tweak your Network<a class="headerlink" href="#exercise-3-3-tweak-your-network" title="Permalink to this headline">¶</a></h3>
<p>You can now play around with the network a little bit to get a feeling of what different parameters are doing. Here are some ideas what you could try:</p>
<ul class="simple">
<li><p>Increase or decrease the number of epochs for training</p></li>
<li><p>Increase or decrease the size of the hidden layer</p></li>
<li><p>Add one additional hidden layer</p></li>
</ul>
<p>Can you get the network to better fit the data?</p>
<div class="section" id="video-14-xor-widget">
<h4>Video 14: XOR Widget<a class="headerlink" href="#video-14-xor-widget" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_remove-input docutils container">
</div>
<p>Exclusive OR (XOR) logical operation gives a true (<code class="docutils literal notranslate"><span class="pre">1</span></code>) output when the number of true inputs is odd. That is, a true output result if one, and only one, of the inputs to the gate is true. If both inputs are false (<code class="docutils literal notranslate"><span class="pre">0</span></code>) or both are true or false output results. Mathematically speaking, XOR represents the inequality function, i.e., the output is true if the inputs are not alike; otherwise, the output is false.</p>
<p>In case of two inputs (<span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>) the following truth table is applied:</p>
<p>\begin{array}{ccc}
X &amp; Y &amp; \text{XOR} \
\hline
0 &amp; 0 &amp; 0 \
0 &amp; 1 &amp; 1 \
1 &amp; 0 &amp; 1 \
1 &amp; 1 &amp; 0 \
\end{array}</p>
<p>Here, with <code class="docutils literal notranslate"><span class="pre">0</span></code>, we denote <code class="docutils literal notranslate"><span class="pre">False</span></code>, and with <code class="docutils literal notranslate"><span class="pre">1</span></code> we denote <code class="docutils literal notranslate"><span class="pre">True</span></code> in boolean terms.</p>
</div>
</div>
<div class="section" id="interactive-demo-3-3-solving-xor">
<h3>Interactive Demo 3.3: Solving XOR<a class="headerlink" href="#interactive-demo-3-3-solving-xor" title="Permalink to this headline">¶</a></h3>
<p>Here we use an open source and famous visualization widget developed by Tensorflow team available <a class="reference external" href="https://github.com/tensorflow/playground">here</a>.</p>
<ul class="simple">
<li><p>Play with the widget and observe that you can not solve the continuous XOR dataset.</p></li>
<li><p>Now add one hidden layer with three units, play with the widget, and set weights by hand to solve this dataset perfectly.</p></li>
</ul>
<p>For the second part, you should set the weights by clicking on the connections and either type the value or use the up and down keys to change it by one increment. You could also do the same for the biases by clicking on the tiny square to each neuron’s bottom left.
Even though there are infinitely many solutions, a neat solution when <span class="math notranslate nohighlight">\(f(x)\)</span> is ReLU is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-105e42f3-8b11-4299-af6a-84f5d0ab1b30">
<span class="eqno">(1)<a class="headerlink" href="#equation-105e42f3-8b11-4299-af6a-84f5d0ab1b30" title="Permalink to this equation">¶</a></span>\[\begin{equation}
  y = f(x_1)+f(x_2)-f(x_1+x_2)
\end{equation}\]</div>
<p>Try to set the weights and biases to implement this function after you played enough :)</p>
<p>###Play with the parameters to solve XOR</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown ###Play with the parameters to solve XOR</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="n">HTML</span><span class="p">(</span><span class="s1">'&lt;iframe width="1020" height="660" src="https://playground.arashash.com/#activation=relu&amp;batchSize=10&amp;dataset=xor&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=&amp;seed=0.91390&amp;showTestData=false&amp;discretize=false&amp;percTrainData=90&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false" allowfullscreen&gt;&lt;/iframe&gt;'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Do you think we can solve the discrete XOR (only 4 possibilities) with only 2 hidden units?</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Do you think we can solve the discrete XOR (only 4 possibilities) with only 2 hidden units?</span>
<span class="n">w1_min_xor</span> <span class="o">=</span> <span class="s1">'Select'</span> <span class="c1">#@param ['Select', 'Yes', 'No']</span>
<span class="k">if</span> <span class="n">w1_min_xor</span> <span class="o">==</span> <span class="s1">'No'</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Correct!"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"How about giving it another try?"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-4-ethics">
<h1>Section 4: Ethics<a class="headerlink" href="#section-4-ethics" title="Permalink to this headline">¶</a></h1>
<p>Let us watch the coded bias movie together and discuss</p>
<div class="section" id="video-15-ethics">
<h2>Video 15: Ethics<a class="headerlink" href="#video-15-ethics" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="bonus">
<h1>Bonus<a class="headerlink" href="#bonus" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-16-be-a-group">
<h2>Video 16: Be a group<a class="headerlink" href="#video-16-be-a-group" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
<div class="section" id="video-17-it-s-a-wrap">
<h2>Video 17: It’s a wrap!<a class="headerlink" href="#video-17-it-s-a-wrap" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
<div class="section" id="video-18-syllabus">
<h2>Video 18: Syllabus<a class="headerlink" href="#video-18-syllabus" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<p>Meet our lecturers:</p>
<p>Week 1: the building blocks</p>
<ul class="simple">
<li><p><a class="reference external" href="https://kordinglab.com">Konrad Kording</a></p></li>
<li><p><a class="reference external" href="https://www.saxelab.org/">Andrew Saxe</a></p></li>
<li><p><a class="reference external" href="https://ganguli-gang.stanford.edu/">Surya Ganguli</a></p></li>
<li><p><a class="reference external" href="http://mitliagkas.github.io/">Ioannis Mitliagkas</a></p></li>
<li><p><a class="reference external" href="https://www.cis.upenn.edu/~ungar/">Lyle Ungar</a></p></li>
</ul>
<p>Week 2: making things work</p>
<ul class="simple">
<li><p><a class="reference external" href="https://webdocs.cs.ualberta.ca/~alona/">Alona Fyshe</a></p></li>
<li><p><a class="reference external" href="https://eckerlab.org/">Alexander Ecker</a></p></li>
<li><p><a class="reference external" href="https://sociology.uchicago.edu/directory/james-evans">James Evans</a></p></li>
<li><p><a class="reference external" href="https://hhexiy.github.io/">He He</a></p></li>
<li><p><a class="reference external" href="https://tnel.ucsd.edu/bio">Vikash Gilja</a> and <a class="reference external" href="https://akashgit.github.io/">Akash Srivastava</a></p></li>
</ul>
<p>Week 3: more magic</p>
<ul class="simple">
<li><p><a class="reference external" href="https://contrastiveconvergence.net/~timothylillicrap/index.php">Tim Lillicrap</a> and <a class="reference external" href="https://www.mcgill.ca/neuro/blake-richards-phd">Blake Richards</a></p></li>
<li><p><a class="reference external" href="http://www.janexwang.com/">Jane Wang</a> and <a class="reference external" href="https://feryal.github.io/">Feryal Behbahani</a></p></li>
<li><p><a class="reference external" href="https://contrastiveconvergence.net/~timothylillicrap/index.php">Tim Lillicrap</a> and <a class="reference external" href="https://www.mcgill.ca/neuro/blake-richards-phd">Blake Richards</a></p></li>
<li><p><a class="reference external" href="https://jovo.me/">Josh Vogelstein</a> and <a class="reference external" href="https://www.vincenzolomonaco.com/">Vincenzo Lamonaco</a></p></li>
</ul>
<p>Now, go to the <a class="reference external" href="https://iclr.cc/virtual/2021/paper_vis.html">visualization of ICLR papers</a>. Read a few abstracts. Look at the various clusters. Where do you see yourself in this map?</p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="appendix">
<h1>Appendix<a class="headerlink" href="#appendix" title="Permalink to this headline">¶</a></h1>
<div class="section" id="official-pytorch-resources">
<h2>Official PyTorch resources:<a class="headerlink" href="#official-pytorch-resources" title="Permalink to this headline">¶</a></h2>
<div class="section" id="tutorials">
<h3>Tutorials<a class="headerlink" href="#tutorials" title="Permalink to this headline">¶</a></h3>
<p>https://pytorch.org/tutorials/</p>
</div>
<div class="section" id="documentation">
<h3>Documentation<a class="headerlink" href="#documentation" title="Permalink to this headline">¶</a></h3>
<p>https://pytorch.org/docs/stable/tensors.html (tensor methods)</p>
<p>https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view  (The view method in particular)</p>
<p>https://pytorch.org/vision/stable/datasets.html (pre-loaded image datasets)</p>
</div>
</div>
<div class="section" id="google-colab-resources">
<h2>Google Colab Resources:<a class="headerlink" href="#google-colab-resources" title="Permalink to this headline">¶</a></h2>
<p>https://research.google.com/colaboratory/faq.html (FAQ including guidance on GPU usage)</p>
</div>
<div class="section" id="books-for-reference">
<h2>Books for reference:<a class="headerlink" href="#books-for-reference" title="Permalink to this headline">¶</a></h2>
<p>https://www.deeplearningbook.org/ (Deep Learning by Ian Goodfellow, Yoshua Bengio and Aaron Courville)</p>
</div>
</div>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W1D1_BasicsAndPytorch/student"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
<div class="prev-next-bottom">
<a class="left-prev" href="../chapter_title.html" id="prev-link" title="previous page">Basics And Pytorch</a>
<a class="right-next" href="../../W1D2_LinearDeepLearning/chapter_title.html" id="next-link" title="next page">Linear Deep Learning</a>
</div>
</div>
</div>
<footer class="footer mt-5 mt-md-0">
<div class="container">
<p>
        
          By Neuromatch<br/>
        
            © Copyright 2021.<br/>
</p>
</div>
</footer>
</main>
</div>
</div>
<script src="../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>
</body>
</html>